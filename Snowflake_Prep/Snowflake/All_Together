STORAGE_COSTS

-- ONLY THE STORAGE COST WILL BE CHARGED 
-- ACCOUNT ADMIN

----- TIME TRAVEL COST -----------

// Storage usage on account level 
SELECT * FROM SNOWFLAKE.ACCOUNT_USAGE.STORAGE_USAGE ORDER BY USAGE_DATE DESC;

// Storage usage on table level 
SELECT * FROM SNOWFLAKE.ACCOUNT_USAGE.TABLE_STORAGE_METRICS;


// Storage usage on table level  - formatted 

SELECT  ID, 
        TABLE_NAME,
        TABLE_SCHEMA,
        TABLE_CATALOG,
        ACTIVE_BYTES / (1024*1024*1024) AS STORAGE_USED_GB,
        TIME_TRAVEL_BYTES / (1024*1024*1024) AS TIME_TRAVEL_STORAGE_USED_GB,
        FAILSAFE_BYTES /  (1024*1024*1024) AS FAILSAFE_GB 
FROM 
        SNOWFLAKE.ACCOUNT_USAGE.TABLE_STORAGE_METRICS
ORDER BY 
        STORAGE_USED_GB DESC , TIME_TRAVEL_STORAGE_USED_GB DESC; 


// Storage usage on Account level formatted 

SELECT USAGE_DATE , 
       STORAGE_BYTES / (1024*1024*1024) AS STORAGE_GB , 
       STAGE_BYTES / (1024*1024*1024) AS STAGE_GB, 
       FAILSAFE_BYTES / (1024*1024*1024) AS FAILSAFE_GB 
FROM 
      SNOWFLAKE.ACCOUNT_USAGE.STORAGE_USAGE 
ORDER BY USAGE_DATE DESC;       
        

        


TABLE_TYPES

-- PERMANENT TABLES ( Permanent data )

SYNTAX      :  CREATE TABLE .... 
TIME TRAVEL :  TIME TRAVEL RETENTION PERIOD 0-90 DAYS 
FAIL SAFE   :  FULL SAFE 
PERSISTANCE :  Until Dropped 


-- TRANSIENT TABLES ( Large tables that does not need to be protected )

SYNTAX      :  CREATE TRANSIENT TABLE .... 
TIME TRAVEL :  0 OR 1 DAY 
FAIL SAFE   :  NOT SUPPORTED 
PERSISTANCE :  Until Dropped 
We can save costs here if our data need not to be protected 



-- TEMPORARY TABLES  ( only for Non-Permanent data )

SYNTAX      :  CREATE TEMPORARY TABLE .... 
TIME TRAVEL :  0 OR 1 DAY 
FAIL SAFE   :  NOT SUPPORTED 
PERSISTANCE :  None . The table persist only in the session 

So by choosing the Table Types , we can manage the storage costs 


-- TABLE TYPES NOTES 

1. Types are also available for other database objects =>  | TABLES | STAGES | SCHEMA | DATABASE |
   ( If database is transient all included objects are transient )

2. For temporary table no naming conflicts with permanent/transient tables 
    Other tables will be effectively hidden ! Relevant for time travel 
    Not visible for other users ! 

3. Not possible to change type of object for existing object 





ZERO COPY CLONING 

ZERO-COPY CLONING 

-- VERY SIMPLE SYNTAX 

CREATE TABLE NEW_TABLE CLONE TABLE_SOURCE 

CLONING CAN ALSO BE DONE FOR => DATABASE | SCHEMA | TABLE | STREAM | FILE FORMAT | SEQUENCE | STAGE | TASK | PIPE 

PIPE CAN ONLY BE CLONED IF IT REFERENCES EXTERNAL STAGES 
STAGES - NAMED INTERNAL STAGES CAN NOT BE CLONED 

Cloning a database or schema will clone all contained objects 

NOTES : 

1. Creates copies of a database , a schema or a table ( Basically a metadata operation ). 
   Both the original & cloned table will reference the same micro-partitions. 
   Both the tables are independent from each other . 
   Question : If the tables referencing the same micro-partitions then how the updated/inserts are independent ? 
   Answer : When a update or Insert happens on any table , new micro-partitions will be craeted . So this way it is storage efficient . 

2. Cloned object is independent from original table 

3. Easy to copy all meta data & improved storage management . 

4. Useful for creating backups for development purposes . 

5. Typically combined with Time Travel 


How about privileges ? 

Privileges will always only be inherited to child objects never to source object itself . 


What privileges are needed ? 

For "table" - SELECT privileges are needed 

For "PIPE | STREAM | TASK"  OWNER privileges are needed 

For "all other objects" USAGE privileges are needed 


Additional Considerations 

1. Load History metadata is not copied ( Loaded data can be loaded again )

2. Cloning from specific point in time is possible 

CREATE TABLE TABLE_NEW 
CLONE TABLE_SOURCE 
BEFORE (TIMESTAMP => 'TIMESTAMP')



//////// PRACTICLES ////////////////////////////////////////////////////////////////////////////

CREATE DATABASE TIMETRAVEL_CLONE CLONE TIMETRAVEL; 



DATA SHARING

DATA SHARING 
-- AVAILABLE FOR ALL SNOWFLAKE EDITIONS 


 Account1   ========= Data is Synchronized ====================>   Account2 (Read only , can't be modified )
(Provider)                                                        (Consumer)   


Note :  
      1. Data will not be copied , it is just the metadata operation ( cloud service layer)
      2. Account1 will have the storage 
      3. Account2 will have it's own "compute resources" . So when Account2 will try to read something , then the respective compute resources of Account2 will be charged . 
      4. Provider & Consumer can be together . Account1(Provider & Consumer) <===> Account2(Provider & Consumer) 


      
Steps : 


1. Create Share : ACCOUNTADMIN role or CREATE SHARE privilges required 
   CREATE SHARE my_share;

2. Grant Privilges to share 
   GRANT USAGE ON DATABASE my_db TO SHARE my_share; 
   GRANT USAGE ON SCHEMA my_schema.my_db TO SHARE my_share; 
   GRANT SELECT ON TABLE my_table.my_schema.my_db TO SHARE my_share; 

3. Add consumer accounts(s)
   ALTER SHARE my_share ADD ACCOUNT 76JGJGJ; 

4. Import Share :  ACCOUNTADMIN role or IMPORT SHARE / CREATE DATABASE privileges required . 
   CREATE DATABASE my_db FROM SHARE my_share; 


What can be shared ?

TABLES | EXTERNAL TABLES | SECURE VIEWS | SECURE MATERIALIZED VIEWS | SECURE UDFs


Share is like a container which has : 
    1. database 
    2. Schema 
    3. Objects 
    4. Accounts(s)
    5. Privileges 

Best Practices : 
     1. Database 
     2. Schema 
     3. Secure views
     4. Accounts(s)
     5. Privileges


Data Sharing with Non-Snowflake Users :
-- Using Reader Account 





////// PRACTICLES ////////////////////////////////////////////////////////////////


-- PREPARATION 

CREATE OR REPLACE DATABASE DATA_SHARE; 
USE DATA_SHARE;

CREATE OR REPLACE FILE FORMAT DATA_SHARE.PUBLIC.FLEFORMAT_AZURE
  TYPE = CSV
  FIELD_DELIMITER = ','
  SKIP_HEADER=1;

  
CREATE OR REPLACE STAGE AZURE_STAGE
     URL = 'azure://azuresnowflakecoupling.blob.core.windows.net/snowflakelearning'
     STORAGE_INTEGRATION = azure_integration
     FILE_FORMAT = DATA_SHARE.PUBLIC.FLEFORMAT_AZURE;


LIST @AZURE_STAGE;     


CREATE OR REPLACE TABLE ORDERS 
(
EMPNO STRING,
ENAME STRING,
JOB STRING,
MGR STRING,
HIREDATE string,
SAL STRING,
COMM STRING,
DEPTNO STRING,
UPDATED_DATE string
);  


COPY INTO ORDERS FROM @AZURE_STAGE
FILES = ('emp.csv')

SELECT * FROM ORDERS ; 


-- NOW suppose we want to share only 5 fields to users without exposing all the data . So we will create a view 

CREATE OR REPLACE VIEW ORDERS_VIEW AS 
SELECT EMPNO,ENAME,JOB,MGR FROM ORDERS; 



NOTE : IT is not possible (recommended ) to share the view to the user . Because though we are sharing the limited data but still the 'text' column can reveal more data . 
SHOW VIEWS LIKE '%ORDERS%' 
The 'text' column will have the whole view defination 



So it is a best practice to share the data using the 'SECURE VIEWS' 

CREATE OR REPLACE SECURE VIEW ORDERS_VIEW_SECURE AS 
(SELECT EMPNO,ENAME,JOB,MGR FROM ORDERS); 

SHOW VIEWS LIKE '%ORDERS%' 




/// CREATING SHARE //// 

-- STEP 1 : CREATE A SHARE OBJECT 
-- You need the ACCOUNTADMIN role 

USE ROLE ACCOUNTADMIN;

-- CREATE SHARE 
CREATE OR REPLACE SHARE ORDERS_SHARE;


-- STEP 2: SET UP THE GRANTS 

GRANT USAGE ON DATABASE DATA_SHARE TO SHARE ORDERS_SHARE; 
GRANT USAGE ON SCHEMA DATA_SHARE.PUBLIC TO SHARE ORDERS_SHARE; 

GRANT SELECT ON VIEW ORDERS_VIEW_SECURE TO SHARE ORDERS_SHARE;

-- VALIDATE GRANTS 
SHOW GRANTS TO SHARE ORDERS_SHARE;





-- CREATE READER ACCOUNT FOR NON-SNOWFLAKE ACCOUNT 

CREATE MANAGED ACCOUNT READER_ACCOUNT 
ADMIN_NAME = read_acc_admin,
ADMIN_PASSWORD = 'Hello@world12456',
TYPE = READER; 

SHOW MANAGED ACCOUNTS;
-- NOTE THE URL 

ALTER SHARE ORDERS_SHARE 
ADD ACCOUNT = FK13264


ALTER SHARE ORDERS_SHARE 
ADD ACCOUNT = FK13264
SHARE_RESTRICTIONS=false 


show shares;


SHOW MANAGED ACCOUNTS;


DATA REPLICATION


DATABASE REPLICATION ( Snlowflake Standard feature)
Replicates a database between accounts within the same organization 

SHARING DATA ACROSS REGIONS AND DIFFERENT CLOUD PROVIDERS ( Cross region Sharing )

Account1 (Region1) =================>  Account2(Region2)

The data is now physically extracted and copied ( Data Tranfer cost will incur)
Data and objects need to be synchronized periodically


STEP 1. Enable replication for source account with ORGADMIN role 

SHOW ORGANISATION ACCOUNTS; 

-- Enable replication for each source and target account in your organization 
SELECT SYSTEM$GLOBAL_ACCOUNT_SET_PARAMETER('organization_name'.<account_name>,
                                           'ENABLE_ACCOUNT_DATABASE_REPLICATION','true');




STEP 2: Promote a Local Database to Primary Database with ACCOUNTADMIN role 

ALTER DATABASE my_dbl ENABLE REPLICATION TO ACCOUNTS myorg.account2,myorg.account3;



STEP 3: Create Replica in consumer account 
CREATE DATABASE my_dbl AS REPLICA OF myorg.account1.my_dbl;

STEP 4: Refresh database 
ALTER DATABASE my_dbl REFRESH; 

NOTE : OWNERSHIP PRIVILEGES ARE NEEDED 

A TASK can be scheduled with this command 


ACCESS CONTROL

ACCESS CONTROL 


TWO ASPECTS 

1. DISCRETIONARY ACCESS CONTROL (DAC)                                     2. ROLE-BASED ACCESS CONTROL 
     - EACH OBJECT HAS AN OWNER                                              - PRIVILEGES 
        (Owner , who created the Object)                                                -- ROLES         
     - OWNER CAN GRANT ACCESS TO THAT OBJECT                                            -------- USERS  
     - EVERY OBJECT IS OWNED BY ONE SINGLE ROLE (OWNERSHIP privileges) 
              -- All privilege per default 
              -- Including GRANT and REVOKE 
              -- Active role in the session
     - Ownership can be transferred 


---------------------------------------------------------------------------
KEY CONCEPTS 
---------------------------------------------------------------------------
SECURABLE OBJECT  -  Access can be granted 
                  -  Access denied unless granted 

PRIVILEGE         -  Defined level of Access

ROLE              -  Entity to which privileges are granted 
                  -  Will be assigned to users |... or other roles 

USER              -  Identity associated with person or program 


-------------------------------------------------------------------------

                                                                  
Example 

We have created a worksheet with a ROLE

ROLE --------> (Creates) SECURABLE OBJECT (owns)
                             |
                              -- PRIVILEGE ---------> (TO) ROLE ------------------- GRANT <role> TO <user> ---------------> (TO) USER (user 1, user2 ...)
                               GRANT <privilege>           -- has                                  
                               ON <object>                 -- Privilege 1 
                               TO <role>                   -- Privilege 2 .. 




ROLES IN SNOWFLAKE 

Roles can be assigned to roles 
Hierarchy of roles 
Privileges are inherited 

ROLE <----------- PRIVILEGE 1 

GRANT <privilege> TO <object> ON <role>

So we GRANT a privilege on a object  to a Role. Also , REVOKE privilege from a ROLE 


-- Roles are assigned to users 
We grant privilege to the Roles and not to the user directly , and then Roles will be granted to the user 

GRANT <role> TO <user>
We cal aslo Grant Roles to other Roles (Multiple Roles can be assigned )
One user can have multiple Roles 

SYSTEM-DEFINED ROLES 
1. Can't be dropped 
2. Privilegs can be added but not revoked 

                 ORGADMIN  - Manages actions on organisational level (Create accounts , View all accounts , View account usage info)

     -------->  ACCOUNTADMIN  <-----  (Top-level Role , most powerful. )
    |                               |                              
    |                               |                              
    |                               |                              
SECURITYADMIN                    SYSADMIN  <-------------------------- Custom Roles    ( Best Practice : Assigned to SYSADMIN)               
    ^                               ^                                      ^ 
    |                               |                                      |
    |                         custome role 1                          custom role 2
USERADMIN                                                                  ^
    ^                                                                      |
    |                                                                 custome role 3  
    |___ PUBLIC 


ACCOUNTADMIN 
      --  Top-Level Role (Most powerful)
      --  Should be to limited number of users 
      --  Contains SECURITYADMIN & SYSADMIN 
      --  Can manage all objects in account
      --  Include share and reader accounts 
      --  Modify account-level parameters 
      --  Manage biling & resource monitors 


SECURITYADMIN
      --  Manage any object grant globally 
      --  MANAGE GRANTS privilege
      --  Create , monitor and manage users & roles 
      --  inherits USERADMIN privileges

      
SYSADMIN
      -- Create warehouses , databases and other objects 
      -- All custom roles should be assigned to 
      -- Can grant privileges on warehouses ,databases , and other objects 

USERADMIN 
      -- Dedicated to user and role management 
      -- CREATE USER & CREATE ROLE privileges 
      -- Can manage users and roles that are owned 

PUBLIC 
     -- Automatically granted per default 
     -- Granted to when no access control needed 
     -- Objects can be owned but are available to everyone 

CUSTOM 
     -- Created by USERADMIN or higher 
     -- CREATE ROLE privilege 
     -- Should be assigned to SYSADMIN 
        Otherwise , SYSADMIN won't be able to manage objects created by these roles '
     -- Custom database roles can be created by owner 


----------------------------------------------------------------------------------------------------

PRIVILIGES 

Define granular level of access 

GRANT <privilege> ON <object> TO <role> 
REVOKE <privilege> ON <object> FROM <role>


VIRTUAL WAREHOUSE =>  MODIFY    |  Enables to alter properties of a warehouse - e.g - resizing 
                      MONITOR   |  Enables to view executed queries by the warehouse 
                      OPERATE   |  Enables to change the state of a warehouse (e.g. suspend and resume)
                      USAGE     |  Enables to use the warehouse and execute queries 
                      OWNERSHIP |  Full control over the warehouse 
                      ALL       |  All privileges apart from OWNERSHIP 

                      
DATABASES            MODIFY          |  Enables to alter properties and settings of a database  
                     MONITOR         |  Enables to perform DESCRIBE command  
                     USAGE           |  Enables to use the database and execute SHOW DATABASES command 
                     REFERENCE_USAGE |  Enables using an object (shared secure view) to reference another object in a diffferent database 
                     ALL             |  All privileges apart from OWNERSHIP 
                     OWNERSHIP       |  Full control over the database
                     CREATE SCHEMA   | Enable creating a schema in the database
                     


DATA_VAULT

--------------------------------------------------------------------
-- setting up the environment
--------------------------------------------------------------------

USE ROLE accountadmin;

CREATE OR REPLACE DATABASE dv_lab;

USE DATABASE dv_lab;

CREATE OR REPLACE WAREHOUSE dv_lab_wh WITH WAREHOUSE_SIZE = 'XSMALL' MIN_CLUSTER_COUNT = 1 MAX_CLUSTER_COUNT = 1 AUTO_SUSPEND = 60 COMMENT = 'Generic WH';
CREATE OR REPLACE WAREHOUSE dv_rdv_wh WITH WAREHOUSE_SIZE = 'XSMALL' MIN_CLUSTER_COUNT = 1 MAX_CLUSTER_COUNT = 1 AUTO_SUSPEND = 60 COMMENT = 'WH for Raw Data Vault object pipelines';
--CREATE OR REPLACE WAREHOUSE dv_bdv_wh WITH WAREHOUSE_SIZE = 'XSMALL' MIN_CLUSTER_COUNT = 1 MAX_CLUSTER_COUNT = 1 AUTO_SUSPEND = 60 COMMENT = 'WH for Business Data Vault object pipelines';
--CREATE OR REPLACE WAREHOUSE dv_id_wh  WITH WAREHOUSE_SIZE = 'XSMALL' MIN_CLUSTER_COUNT = 1 MAX_CLUSTER_COUNT = 1 AUTO_SUSPEND = 60 COMMENT = 'WH for information delivery object pipelines';

USE WAREHOUSE dv_lab_wh;

CREATE OR REPLACE SCHEMA l00_stg COMMENT = 'Schema for Staging Area objects';
CREATE OR REPLACE SCHEMA l10_rdv COMMENT = 'Schema for Raw Data Vault objects';
CREATE OR REPLACE SCHEMA l20_bdv COMMENT = 'Schema for Business Data Vault objects';
CREATE OR REPLACE SCHEMA l30_id  COMMENT = 'Schema for Information Delivery objects';



--------------------------------------------------------------------
-- setting up staging area
--------------------------------------------------------------------

USE SCHEMA l00_stg;

CREATE OR REPLACE TABLE stg_nation 
AS 
SELECT src.*
     , CURRENT_TIMESTAMP()          ldts 
     , 'Static Reference Data'      rscr 
  FROM snowflake_sample_data.tpch_sf10.nation src;

select * from stg_nation;
  
CREATE OR REPLACE TABLE stg_region
AS 
SELECT src.*
     , CURRENT_TIMESTAMP()          ldts 
     , 'Static Reference Data'      rscr 
  FROM snowflake_sample_data.tpch_sf10.region src;

select * from stg_region;



CREATE OR REPLACE TABLE stg_customer
(
  raw_json                VARIANT
, filename                STRING   NOT NULL
, file_row_seq            NUMBER   NOT NULL
, ldts                    STRING   NOT NULL
, rscr                    STRING   NOT NULL
);

CREATE OR REPLACE TABLE stg_orders
(
  o_orderkey              NUMBER
, o_custkey               NUMBER  
, o_orderstatus           STRING
, o_totalprice            NUMBER  
, o_orderdate             DATE
, o_orderpriority         STRING
, o_clerk                 STRING
, o_shippriority          NUMBER
, o_comment               STRING
, filename                STRING   NOT NULL
, file_row_seq            NUMBER   NOT NULL
, ldts                    STRING   NOT NULL
, rscr                    STRING   NOT NULL
);



CREATE OR REPLACE STREAM stg_customer_strm ON TABLE stg_customer;
CREATE OR REPLACE STREAM stg_orders_strm ON TABLE stg_orders;

CREATE OR REPLACE STAGE customer_data FILE_FORMAT = (TYPE = JSON);
CREATE OR REPLACE STAGE orders_data   FILE_FORMAT = (TYPE = CSV) ;




COPY INTO @customer_data 
FROM
(SELECT object_construct(*)
  FROM snowflake_sample_data.tpch_sf10.customer limit 10
) 
INCLUDE_QUERY_ID=TRUE;

select * from @customer_data;

COPY INTO @orders_data 
FROM
(SELECT *
  FROM snowflake_sample_data.tpch_sf10.orders limit 1000
) 
INCLUDE_QUERY_ID=TRUE;

select * from @orders_data;



list @customer_data;
SELECT METADATA$FILENAME,$1 FROM @customer_data; 



CREATE OR REPLACE PIPE stg_orders_pp 
AS 
COPY INTO stg_orders 
FROM
(
SELECT $1,$2,$3,$4,$5,$6,$7,$8,$9 
     , metadata$filename
     , metadata$file_row_number
     , CURRENT_TIMESTAMP()
     , 'Orders System'
  FROM @orders_data
);

CREATE OR REPLACE PIPE stg_customer_pp 
--AUTO_INGEST = TRUE
--aws_sns_topic = 'arn:aws:sns:mybucketdetails'
AS 
COPY INTO stg_customer
FROM 
(
SELECT $1
     , metadata$filename
     , metadata$file_row_number
     , CURRENT_TIMESTAMP()
     , 'Customers System'
  FROM @customer_data
);


ALTER PIPE stg_customer_pp REFRESH;


ALTER PIPE stg_orders_pp   REFRESH;



SELECT 'stg_customer', count(1) FROM stg_customer
UNION ALL
SELECT 'stg_orders', count(1) FROM stg_orders
UNION ALL
SELECT 'stg_orders_strm', count(1) FROM stg_orders_strm
UNION ALL
SELECT 'stg_customer_strm', count(1) FROM stg_customer_strm
;




CREATE OR REPLACE VIEW stg_customer_strm_outbound AS 
SELECT src.*
     , raw_json:C_CUSTKEY::NUMBER           c_custkey
     , raw_json:C_NAME::STRING              c_name
     , raw_json:C_ADDRESS::STRING           c_address
     , raw_json:C_NATIONKEY::NUMBER         C_nationcode
     , raw_json:C_PHONE::STRING             c_phone
     , raw_json:C_ACCTBAL::NUMBER           c_acctbal
     , raw_json:C_MKTSEGMENT::STRING        c_mktsegment
     , raw_json:C_COMMENT::STRING           c_comment     
--------------------------------------------------------------------
-- derived business key
--------------------------------------------------------------------
     , SHA1_BINARY(UPPER(TRIM(c_custkey)))  sha1_hub_customer     
     , SHA1_BINARY(UPPER(ARRAY_TO_STRING(ARRAY_CONSTRUCT( 
                                              NVL(TRIM(c_name)       ,'-1')
                                            , NVL(TRIM(c_address)    ,'-1')              
                                            , NVL(TRIM(c_nationcode) ,'-1')                 
                                            , NVL(TRIM(c_phone)      ,'-1')            
                                            , NVL(TRIM(c_acctbal)    ,'-1')               
                                            , NVL(TRIM(c_mktsegment) ,'-1')                 
                                            , NVL(TRIM(c_comment)    ,'-1')               
                                            ), '^')))  AS customer_hash_diff
  FROM stg_customer_strm src
;

select * from stg_customer_strm_outbound;

CREATE OR REPLACE VIEW stg_order_strm_outbound AS 
SELECT src.*
--------------------------------------------------------------------
-- derived business key
--------------------------------------------------------------------
     , SHA1_BINARY(UPPER(TRIM(o_orderkey)))             sha1_hub_order
     , SHA1_BINARY(UPPER(TRIM(o_custkey)))              sha1_hub_customer  
     , SHA1_BINARY(UPPER(ARRAY_TO_STRING(ARRAY_CONSTRUCT( NVL(TRIM(o_orderkey)       ,'-1')
                                                        , NVL(TRIM(o_custkey)        ,'-1')
                                                        ), '^')))  AS sha1_lnk_customer_order             
     , SHA1_BINARY(UPPER(ARRAY_TO_STRING(ARRAY_CONSTRUCT( NVL(TRIM(o_orderstatus)    , '-1')         
                                                        , NVL(TRIM(o_totalprice)     , '-1')        
                                                        , NVL(TRIM(o_orderdate)      , '-1')       
                                                        , NVL(TRIM(o_orderpriority)  , '-1')           
                                                        , NVL(TRIM(o_clerk)          , '-1')    
                                                        , NVL(TRIM(o_shippriority)   , '-1')          
                                                        , NVL(TRIM(o_comment)        , '-1')      
                                                        ), '^')))  AS order_hash_diff     
  FROM stg_orders_strm src
;

select * from stg_order_strm_outbound;





--------------------------------------------------------------------
-- setting up RDV
--------------------------------------------------------------------

USE SCHEMA l10_rdv;

-- hubs

CREATE OR REPLACE TABLE hub_customer 
( 
  sha1_hub_customer       BINARY    NOT NULL   
, c_custkey               NUMBER    NOT NULL                                                                                 
, ldts                    TIMESTAMP NOT NULL
, rscr                    STRING    NOT NULL
, CONSTRAINT pk_hub_customer        PRIMARY KEY(sha1_hub_customer)
);                                     

CREATE OR REPLACE TABLE hub_order 
( 
  sha1_hub_order          BINARY    NOT NULL   
, o_orderkey              NUMBER    NOT NULL                                                                                 
, ldts                    TIMESTAMP NOT NULL
, rscr                    STRING    NOT NULL
, CONSTRAINT pk_hub_order           PRIMARY KEY(sha1_hub_order)
);                                     

-- sats

CREATE OR REPLACE TABLE sat_customer 
( 
  sha1_hub_customer      BINARY    NOT NULL   
, ldts                   TIMESTAMP NOT NULL
, c_name                 STRING
, c_address              STRING
, c_phone                STRING 
, c_acctbal              NUMBER
, c_mktsegment           STRING    
, c_comment              STRING
, nationcode             NUMBER
, hash_diff              BINARY    NOT NULL
, rscr                   STRING    NOT NULL  
, CONSTRAINT pk_sat_customer       PRIMARY KEY(sha1_hub_customer, ldts)
, CONSTRAINT fk_sat_customer       FOREIGN KEY(sha1_hub_customer) REFERENCES hub_customer
);                                     

CREATE OR REPLACE TABLE sat_order 
( 
  sha1_hub_order         BINARY    NOT NULL   
, ldts                   TIMESTAMP NOT NULL
, o_orderstatus          STRING   
, o_totalprice           NUMBER
, o_orderdate            DATE
, o_orderpriority        STRING
, o_clerk                STRING    
, o_shippriority         NUMBER
, o_comment              STRING
, hash_diff              BINARY    NOT NULL
, rscr                   STRING    NOT NULL   
, CONSTRAINT pk_sat_order PRIMARY KEY(sha1_hub_order, ldts)
, CONSTRAINT fk_sat_order FOREIGN KEY(sha1_hub_order) REFERENCES hub_order
);   

-- links

CREATE OR REPLACE TABLE lnk_customer_order
(
  sha1_lnk_customer_order BINARY     NOT NULL   
, sha1_hub_customer       BINARY 
, sha1_hub_order          BINARY 
, ldts                    TIMESTAMP  NOT NULL
, rscr                    STRING     NOT NULL  
, CONSTRAINT pk_lnk_customer_order  PRIMARY KEY(sha1_lnk_customer_order)
, CONSTRAINT fk1_lnk_customer_order FOREIGN KEY(sha1_hub_customer) REFERENCES hub_customer
, CONSTRAINT fk2_lnk_customer_order FOREIGN KEY(sha1_hub_order)    REFERENCES hub_order
);

-- ref data

CREATE OR REPLACE TABLE ref_region
( 
  regioncode            NUMBER 
, ldts                  TIMESTAMP
, rscr                  STRING NOT NULL
, r_name                STRING
, r_comment             STRING
, CONSTRAINT PK_REF_REGION PRIMARY KEY (REGIONCODE)                                                                             
)
AS 
SELECT r_regionkey
     , ldts
     , rscr
     , r_name
     , r_comment
  FROM l00_stg.stg_region;

CREATE OR REPLACE TABLE ref_nation 
( 
  nationcode            NUMBER 
, regioncode            NUMBER 
, ldts                  TIMESTAMP
, rscr                  STRING NOT NULL
, n_name                STRING
, n_comment             STRING
, CONSTRAINT pk_ref_nation PRIMARY KEY (nationcode)                                                                             
, CONSTRAINT fk_ref_region FOREIGN KEY (regioncode) REFERENCES ref_region(regioncode)  
)
AS 
SELECT n_nationkey
     , n_regionkey
     , ldts
     , rscr
     , n_name
     , n_comment
  FROM l00_stg.stg_nation;  



CREATE OR REPLACE TASK customer_strm_tsk
  WAREHOUSE = dv_rdv_wh
  SCHEDULE = '1 minute'
WHEN
  SYSTEM$STREAM_HAS_DATA('L00_STG.STG_CUSTOMER_STRM')
AS 
INSERT ALL
WHEN (SELECT COUNT(1) FROM hub_customer tgt WHERE tgt.sha1_hub_customer = src_sha1_hub_customer) = 0
THEN INTO hub_customer  
( sha1_hub_customer
, c_custkey
, ldts
, rscr
)  
VALUES 
( src_sha1_hub_customer
, src_c_custkey
, src_ldts
, src_rscr
)  
WHEN (SELECT COUNT(1) FROM sat_customer tgt WHERE tgt.sha1_hub_customer = src_sha1_hub_customer AND tgt.hash_diff = src_customer_hash_diff) = 0
THEN INTO sat_customer  
(
  sha1_hub_customer  
, ldts              
, c_name            
, c_address         
, c_phone           
, c_acctbal         
, c_mktsegment      
, c_comment         
, nationcode        
, hash_diff         
, rscr              
)  
VALUES 
(
  src_sha1_hub_customer  
, src_ldts              
, src_c_name            
, src_c_address         
, src_c_phone           
, src_c_acctbal         
, src_c_mktsegment      
, src_c_comment         
, src_nationcode        
, src_customer_hash_diff         
, src_rscr              
)
SELECT sha1_hub_customer   src_sha1_hub_customer
     , c_custkey           src_c_custkey
     , c_name              src_c_name
     , c_address           src_c_address
     , c_nationcode        src_nationcode
     , c_phone             src_c_phone
     , c_acctbal           src_c_acctbal
     , c_mktsegment        src_c_mktsegment
     , c_comment           src_c_comment    
     , customer_hash_diff  src_customer_hash_diff
     , ldts                src_ldts
     , rscr                src_rscr
  FROM l00_stg.stg_customer_strm_outbound src
;


CREATE OR REPLACE TASK order_strm_tsk
  WAREHOUSE = dv_rdv_wh
  SCHEDULE = '1 minute'
WHEN
  SYSTEM$STREAM_HAS_DATA('L00_STG.STG_ORDERS_STRM')
AS 
INSERT ALL
WHEN (SELECT COUNT(1) FROM hub_order tgt WHERE tgt.sha1_hub_order = src_sha1_hub_order) = 0
THEN INTO hub_order  
( sha1_hub_order
, o_orderkey
, ldts
, rscr
)  
VALUES 
( src_sha1_hub_order
, src_o_orderkey
, src_ldts
, src_rscr
)  
WHEN (SELECT COUNT(1) FROM sat_order tgt WHERE tgt.sha1_hub_order = src_sha1_hub_order AND tgt.hash_diff = src_order_hash_diff) = 0
THEN INTO sat_order  
(
  sha1_hub_order  
, ldts              
, o_orderstatus  
, o_totalprice   
, o_orderdate    
, o_orderpriority
, o_clerk        
, o_shippriority 
, o_comment              
, hash_diff         
, rscr              
)  
VALUES 
(
  src_sha1_hub_order  
, src_ldts              
, src_o_orderstatus  
, src_o_totalprice   
, src_o_orderdate    
, src_o_orderpriority
, src_o_clerk        
, src_o_shippriority 
, src_o_comment      
, src_order_hash_diff         
, src_rscr              
)
WHEN (SELECT COUNT(1) FROM lnk_customer_order tgt WHERE tgt.sha1_lnk_customer_order = src_sha1_lnk_customer_order) = 0
THEN INTO lnk_customer_order  
(
  sha1_lnk_customer_order  
, sha1_hub_customer              
, sha1_hub_order  
, ldts
, rscr              
)  
VALUES 
(
  src_sha1_lnk_customer_order
, src_sha1_hub_customer
, src_sha1_hub_order  
, src_ldts              
, src_rscr              
)
SELECT sha1_hub_order          src_sha1_hub_order
     , sha1_lnk_customer_order src_sha1_lnk_customer_order
     , sha1_hub_customer       src_sha1_hub_customer
     , o_orderkey              src_o_orderkey
     , o_orderstatus           src_o_orderstatus  
     , o_totalprice            src_o_totalprice   
     , o_orderdate             src_o_orderdate    
     , o_orderpriority         src_o_orderpriority
     , o_clerk                 src_o_clerk        
     , o_shippriority          src_o_shippriority 
     , o_comment               src_o_comment      
     , order_hash_diff         src_order_hash_diff
     , ldts                    src_ldts
     , rscr                    src_rscr
  FROM l00_stg.stg_order_strm_outbound src;    

ALTER TASK customer_strm_tsk RESUME;  
ALTER TASK order_strm_tsk    RESUME;  



SELECT *
  FROM table(information_schema.task_history())
  ORDER BY scheduled_time DESC;



SELECT 'hub_customer', count(1) FROM hub_customer
UNION ALL
SELECT 'hub_order', count(1) FROM hub_order
UNION ALL
SELECT 'sat_customer', count(1) FROM sat_customer
UNION ALL
SELECT 'sat_order', count(1) FROM sat_order
UNION ALL
SELECT 'lnk_customer_order', count(1) FROM lnk_customer_order
UNION ALL
SELECT 'l00_stg.stg_customer_strm_outbound', count(1) FROM l00_stg.stg_customer_strm_outbound
UNION ALL
SELECT 'l00_stg.stg_order_strm_outbound', count(1) FROM l00_stg.stg_order_strm_outbound;



USE SCHEMA l20_bdv;

CREATE OR REPLACE VIEW sat_customer_bv
AS
SELECT rsc.sha1_hub_customer  
     , rsc.ldts                   
     , rsc.c_name                 
     , rsc.c_address              
     , rsc.c_phone                 
     , rsc.c_acctbal              
     , rsc.c_mktsegment               
     , rsc.c_comment              
     , rsc.nationcode             
     , rsc.rscr 
     -- derived 
     , rrn.n_name                    nation_name
     , rrr.r_name                    region_name
  FROM l10_rdv.sat_customer          rsc
  LEFT OUTER JOIN l10_rdv.ref_nation rrn
    ON (rsc.nationcode = rrn.nationcode)
  LEFT OUTER JOIN l10_rdv.ref_region rrr
    ON (rrn.regioncode = rrr.regioncode)
;


CREATE OR REPLACE TABLE sat_order_bv
( 
  sha1_hub_order         BINARY    NOT NULL   
, ldts                   TIMESTAMP NOT NULL
, o_orderstatus          STRING   
, o_totalprice           NUMBER
, o_orderdate            DATE
, o_orderpriority        STRING
, o_clerk                STRING    
, o_shippriority         NUMBER
, o_comment              STRING  
, hash_diff              BINARY    NOT NULL
, rscr                   STRING    NOT NULL   
-- additional attributes
, order_priority_bucket  STRING
, CONSTRAINT pk_sat_order PRIMARY KEY(sha1_hub_order, ldts)
, CONSTRAINT fk_sat_order FOREIGN KEY(sha1_hub_order) REFERENCES l10_rdv.hub_order
)
AS 
SELECT sha1_hub_order 
     , ldts           
     , o_orderstatus  
     , o_totalprice   
     , o_orderdate    
     , o_orderpriority
     , o_clerk        
     , o_shippriority 
     , o_comment      
     , hash_diff      
     , rscr 
     -- derived additional attributes
     , CASE WHEN o_orderpriority IN ('2-HIGH', '1-URGENT')             AND o_totalprice >= 200000 THEN 'Tier-1'
            WHEN o_orderpriority IN ('3-MEDIUM', '2-HIGH', '1-URGENT') AND o_totalprice BETWEEN 150000 AND 200000 THEN 'Tier-2'  
            ELSE 'Tier-3'
       END order_priority_bucket
  FROM l10_rdv.sat_order;



CREATE OR REPLACE STREAM l10_rdv.sat_order_strm ON TABLE l10_rdv.sat_order; 

ALTER TASK l10_rdv.order_strm_tsk SUSPEND;

CREATE OR REPLACE TASK l10_rdv.hub_order_strm_sat_order_bv_tsk
  WAREHOUSE = dv_rdv_wh
  AFTER l10_rdv.order_strm_tsk
AS 
INSERT INTO l20_bdv.sat_order_bv
SELECT   
  sha1_hub_order 
, ldts           
, o_orderstatus  
, o_totalprice   
, o_orderdate    
, o_orderpriority
, o_clerk        
, o_shippriority 
, o_comment      
, hash_diff      
, rscr 
-- derived additional attributes
, CASE WHEN o_orderpriority IN ('2-HIGH', '1-URGENT')             AND o_totalprice >= 200000 THEN 'Tier-1'
       WHEN o_orderpriority IN ('3-MEDIUM', '2-HIGH', '1-URGENT') AND o_totalprice BETWEEN 150000 AND 200000 THEN 'Tier-2'  
       ELSE 'Tier-3'
  END order_priority_bucket
FROM sat_order_strm;

ALTER TASK l10_rdv.hub_order_strm_sat_order_bv_tsk RESUME;
ALTER TASK l10_rdv.order_strm_tsk RESUME;


USE SCHEMA l00_stg;

COPY INTO @orders_data 
FROM
(SELECT *
  FROM snowflake_sample_data.tpch_sf10.orders limit 1000
) 
INCLUDE_QUERY_ID=TRUE;

ALTER PIPE stg_orders_pp   REFRESH;





SELECT 'l00_stg.stg_orders', count(1) FROM l00_stg.stg_orders
UNION ALl
SELECT 'l00_stg.stg_orders_strm', count(1) FROM l00_stg.stg_orders_strm
UNION ALl
SELECT 'l10_rdv.sat_order', count(1) FROM l10_rdv.sat_order
UNION ALl
SELECT 'l10_rdv.sat_order_strm', count(1) FROM l10_rdv.sat_order_strm
UNION ALL
SELECT 'l20_bdv.sat_order_bv', count(1) FROM l20_bdv.sat_order_bv;





select * from SNOWFLAKE.ACCOUNT_USAGE.QUERY_HISTORY; 

  

JSON PARSING

//  EXPERIMENT 1 => JSON & LATERAL FLATTEN FUNCTIONS  

USE DATABASE JSON_DATA;

USE SCHEMA JSON_SCHEMA;


-- STEP 1 :  CREATE A SNOWFLAKE INTERNAL NAMED STAGE 

CREATE OR REPLACE STAGE my_internal_stage_for_json;

-- STEP 2 :  LOAD DATA FROM LOCAL MACHINE TO THIS INTERNALE STAGE 
-- USING SNOWSQL EXECUTE THIS COMMAND :  
-- ****   PUT file:///Users/mukesh/desktop/authors.json @~/my_internal_stage_for_json;  ****

-- STEP 3 : VERIFY THE FILE IN INTERNAL STAGE USNG LIST COMMAND 

LIST @~/my_internal_stage_for_json;

-- STEP 4 : LOADING JSON DATA FROM INTERNAL STAGE INTO DATABASE TABLE 

-- CREATE JSON FILE FORMAT AS 
CREATE OR REPLACE FILE FORMAT JSON_TYPE 
     TYPE = 'json'
     STRIP_OUTER_ARRAY = TRUE;

-- STEP 5 : CREATE DATABASE TABLE TO LOAD JSON DATA 

CREATE OR REPLACE TABLE AUTHORS 
(
   JSON_DATA VARIANT
);

-- STEP 6 : LOAD DATA FROM INTERNAL STAGE TO THIS DATABASE TABLE USING COPY COMMAND 

COPY INTO AUTHORS 
FROM @~/my_internal_stage_for_json
FILE_FORMAT = (FORMAT_NAME = JSON_TYPE);

-- STEP 7 : QUERING JSON DATA FROM THIS TABLE 

SELECT * FROM AUTHORS;

-- STEP 8 : QUERY SOME INDIVIDUAL COLUMNS 
SELECT JSON_DATA:AuthorName, JSON_DATA:Category from AUTHORS;

-- STEP 9 : THE DATA IN CATEGORY COLUMN CAN BE FURTHER DRILLED DOWN AS 

SELECT JSON_DATA:AuthorName,
       JSON_DATA:Category[0]:CategoryName,
       JSON_DATA:Category[1]:CategoryName
FROM AUTHORS;       

-- Step 10 : Remove the outer Quotes 

SELECT JSON_DATA:AuthorName::string,
       JSON_DATA:Category[0]:CategoryName::string,
       JSON_DATA:Category[1]:CategoryName::string
FROM AUTHORS;       

-- STEP 11 : MORE DETAILS CAN BE FETCHED AS 
SELECT 
    JSON_DATA:AuthorName::string,
    JSON_DATA:Category[0]:CategoryName::string,
    JSON_DATA:Category[0]:Genre[0]:GenreName::string,
    JSON_DATA:Category[0]:Genre[1]:GenreName::string,
    JSON_DATA:Category[1]:CategoryName::string,
    JSON_DATA:Category[1]:Genre[0]:GenreName::string,
    JSON_DATA:Category[1]:Genre[1]:GenreName::string
FROM Authors;

-- UNFORTUNATELY THE ABOVE APPROACH IS NOT IDEAL BECAUSE AS THE DATA INCREASES , WE NEED TO ADD ADDITIONAL LEVELS 
-- OF CATEGORY IN THE QUERY STATEMENT 

//// **********   FLATTENING ARRAYS IN JSON DATA **********  //////  

-- Flattening is a process of unpacking the semi-structured data into a columnar format by converting arrays into -- --different rows of data.

SELECT JSON_DATA:AuthorName::string AS Author, 
       VALUE:CategoryName::string as CategoryName,
       VALUE:Genre::string as Genre
FROM AUTHORS,
LATERAL FLATTEN(INPUT => JSON_DATA:Category); 

-- We have more columns to flatten Category , Genre and Novel to get the desired output . 

-- Also note , Novel array is present inside Genre array which is present inside Category array . 
-- So the flattened array output VALUE becomes input for the array present inside it . 


SELECT 
       JSON_DATA:AuthorName::string as AuthorName, 
       Flatten_Category.VALUE:CategoryName::string AS Category_Name,
       Flatten_Genre.VALUE:GenreName::string AS Genre_Name,
       Flatten_Novel.VALUE:Novel::string AS Novel_Name,
       Flatten_Novel.VALUE:Sales::string AS Sales_in_Millions
FROM AUTHORS 
,LATERAL FLATTEN(INPUT => JSON_DATA:Category) AS Flatten_Category
,LATERAL FLATTEN(INPUT => Flatten_Category.VALUE:Genre) AS Flatten_Genre
,LATERAL FLATTEN(INPUT => Flatten_Genre.VALUE:Novel) AS Flatten_Novel
;

//////////////////////////////////////////////////////////////////////////////////////////////////

-- Reading source : https://thinketl.com/how-to-load-and-query-json-data-in-snowflake/ 



STORED PROCEDURE

CREATE OR REPLACE DATABASE STORED_PROCS;
CREATE OR REPLACE SCHEMA STORED_PROCS.PROC_SCHEMA;

USE STORED_PROCS;

CREATE OR REPLACE PROCEDURE TEST_PROC (TABLE_NAME STRING)
RETURNS VARCHAR
LANGUAGE SQL
AS
$$
    DECLARE 
         MESSAGE VARCHAR;
         ERROR_MESSAGE VARCHAR;
         
    BEGIN 
         CREATE OR REPLACE TABLE TEST_TABLE(ID INT,NAME VARCHAR);
         INSERT INTO TEST_TABLE VALUES (1,'MUKESH');
         SELECT COUNT(*) FROM TEST_TABLE;
         MESSAGE := 'TABLE IS CREATED SUCCESSFULLY';
         RETURN MESSAGE;
    
    END;     
$$
;


CALL TEST_PROC('TEST');

CREATE OR REPLACE PROCEDURE TEST_EXCEPTION()
RETURNS VARCHAR
LANGUAGE SQL
AS
$$
DECLARE
  RESULT VARCHAR;
  EXCEPTION_1 EXCEPTION (-20001, 'I caught the expected exception.');
  EXCEPTION_2 EXCEPTION (-20002, 'Not the expected exception!');
BEGIN
  RESULT := 'If you see this, I did not catch any exception.';
  IF (TRUE) THEN
    RAISE EXCEPTION_1;
  END IF;
  RETURN RESULT;
EXCEPTION
  WHEN EXCEPTION_2 THEN
    RETURN SQLERRM;
  WHEN EXCEPTION_1 THEN
    RETURN SQLERRM;
END;
$$
;

CALL TEST_EXCEPTION();


CREATE OR REPLACE PROCEDURE NEW_DB_OBJECT()
RETURNS VARCHAR
LANGUAGE SQL
AS
$$
DECLARE 
       DB_NAME := 'SUYAL_TEST_DB';
       SCHEMA_NAME := 'SUYAL_TEST_SCHEMA';
       EXCEPTION_DB EXCEPTION (-20001,'DB CREATE EXCEPTION');
BEGIN 
     CREATE OR REPLACE DATABASE :DB_NAME;
END;
$$

CALL NEW_DB_OBJECT()

USE SUYAL_TEST_DB;




// javascript 

create or replace procedure getRowCount(table_name VARCHAR)
    returns float 
    not null
    language javascript
    as
    $$
    var row_count = 0;
    // Dynamically compose the SQL statement to execute.
    var sqlcmd = "create or replace table "+ TABLE_NAME + "(id int , name varchar)";
    var sqlcmd2 = "insert into "+TABLE_NAME+ " VALUES (1,'MUKESH')";
    // Prepare statement.
    var stmt = snowflake.createStatement(
           {
           sqlText: sqlcmd
           }
        );
    var stmt2 = snowflake.createStatement(
           { 
           sqlText: sqlcmd2
           
           }
    );    
	// Execute Statement
    var res = stmt.execute();
    var res = stmt2.execute();
    //res.next();
    //row_count = res.getColumnValue(1);
    // return row_count;
    $$
    ;


call getRowCount('suyalji85');


select * from suyalji85


call NEW_DB_OBJECT('abc1','abc2','abc3');

use database STORED_PROCS;

CREATE OR REPLACE PROCEDURE NEW_DB_OBJECT(DB_NAME VARCHAR,SCHEMA_NAME VARCHAR,TABLE_NAME VARCHAR)
RETURNS VARCHAR
LANGUAGE JAVASCRIPT 
AS
$$
    var row_count = 0;
    // Dynamicall ompose the SQL statement to execute.
    var create_db = "create or replace database "+ DB_NAME;
    var create_schema = "create or replace schema "+ DB_NAME +"."+ SCHEMA_NAME;
    var use_db = "use "+ DB_NAME;
    var use_schema = "use "+ SCHEMA_NAME;
    var create_table = "create or replace table "+ TABLE_NAME + "(ID INT , NAME VARCHAR)";

    var insert_dummy = "insert into "+ TABLE_NAME + " VALUES (1,'MUKESH')";
    
    
    // Prepare statement.
    var db_create1 = snowflake.createStatement(
           {sqlText: create_db}
        );
        
    var schema_create2 = snowflake.createStatement(
           { sqlText: create_schema }
    );    

    var use_db_create3 = snowflake.createStatement(
           { sqlText: use_db }
    ); 
    var use_schema_create4 = snowflake.createStatement(
           { sqlText: use_schema }
    );    
    var table_create5 = snowflake.createStatement(
           { sqlText: create_table }
    );    
    var dummy_table_create6 = snowflake.createStatement(
           { sqlText: insert_dummy }
    );    



    
	// Execute Statement
    var db = db_create1.execute();
    var sch = schema_create2.execute();
    //var dbuse = use_db_create3.execute();
    //var sch_create = use_schema_create4.execute();
    var tbl_create = table_create5.execute();
    var dum = dummy_table_create6.execute();
   
    //res.next();
    //row_count = res.getColumnValue(1);
    // return row_count;
    $$
    ;




-- USING JAVA SCRIPT


-- Stored Procedure to generate merge statement
-- Proc Name : MAIN_PROC
-- Input Parameters
--      A. Source Table Name
--      B. Target Table Name
--		C. Source PK as SRC_FILTER
--		D. Target PK as TGT_FILTER
-- Author : Sriganesh Palani

CALL main_proc('APOLLO_JSON')

USE DATABASE APOLLO;
USE SCHEMA APOLLO_VARIANT;

CREATE OR REPLACE PROCEDURE main_proc(TABLE_NAME STRING)
RETURNS VARIANT
LANGUAGE JAVASCRIPT
EXECUTE AS CALLER
AS
$$
	// variable Declaration
	var formatted_pk = [];
    var stmt = snowflake.createStatement(
		{
			sqlText:'INSERT INTO APOLLO.APOLLO_SCHEMA.STG_STATS_REALASSET SELECT JSON_DATA:SessionId,JSON_DATA:ReportId,JSON_DATA:JobId,JSON_DATA:JobStatus,JSON_DATA:Status,JSON_DATA:Message FROM APOLLO.APOLLO_SCHEMA.:1)',binds:[TABLE_NAME]
		}
	);
	
	try{
		var result = stmt.execute();
		result.next();
		formatted_pk.push(result.getColumnValue(1))
        return "Successfully loaded"
	}
	
	// Exception handling part
	catch(err){
		return "Error :" + err;
	}
 $$

 //


CREATE OR REPLACE PROCEDURE find_invoice_by_id_via_execute_immediate(id VARCHAR)
RETURNS string
LANGUAGE SQL
AS
DECLARE
  select_statement VARCHAR;
  res RESULTSET;
BEGIN
  select_statement := 'SELECT * FROM ' || id;
  res := (EXECUTE IMMEDIATE :select_statement);
  RETURN TABLE(res);
END;

find_invoice_by_id_via_execute_immediate('APOLLO_VARIANT')


APOLLO JSON

//  EXPERIMENT 1 => JSON & LATERAL FLATTEN FUNCTIONS  

CREATE DATABASE APOLLO; 
CREATE SCHEMA APOLLO_SCHEMA;

USE DATABASE APOLLO;
USE SCHEMA APOLLO_SCHEMA;

-- STEP 1 :  CREATE A SNOWFLAKE INTERNAL NAMED STAGE 

CREATE OR REPLACE STAGE my_internal_stage_for_apollo_json;

-- SnowSQL : PUT file:///Users/mukesh/desktop/apollo_json.json @my_internal_stage_for_apollo_json; 

-- STEP 3 : VERIFY THE FILE IN INTERNAL STAGE USNG LIST COMMAND                                                                                                

LIST @my_internal_stage_for_apollo_json;


-- rm @my_internal_stage_for_apollo_json;

-- STEP 4 : LOADING JSON DATA FROM INTERNAL STAGE INTO DATABASE TABLE 

-- CREATE JSON FILE FORMAT AS 
CREATE OR REPLACE FILE FORMAT JSON_TYPE 
     TYPE = 'json'
     STRIP_OUTER_ARRAY = TRUE;

-- STEP 5 : CREATE DATABASE TABLE TO LOAD JSON DATA 

CREATE OR REPLACE TABLE APOLLO_VARIANT   -- REALAPOLLO_LOAN
(  UID VARCHAR(16777216) DEFAULT UUID_STRING(),
   ROW_LOAD_TIME_STAMP TIMESTAMP_NTZ(9) DEFAULT CAST(CONVERT_TIMEZONE('America/New_York', CAST(CURRENT_TIMESTAMP() AS TIMESTAMP_TZ(9))) AS TIMESTAMP_NTZ(9)),
   JSON_DATA VARIANT
);


CREATE OR REPLACE TABLE RAW_JSON   -- REALAPOLLO_LOAN
(  UID VARCHAR(16777216) DEFAULT UUID_STRING(),
   ROW_LOAD_TIME_STAMP TIMESTAMP_NTZ(9) DEFAULT CAST(CONVERT_TIMEZONE('America/New_York', CAST(CURRENT_TIMESTAMP() AS TIMESTAMP_TZ(9))) AS TIMESTAMP_NTZ(9)),
   JSON_DATA VARIANT
);


-- STEP 6 : LOAD DATA FROM INTERNAL STAGE TO THIS DATABASE TABLE USING COPY COMMAND 

COPY INTO APOLLO_VARIANT(JSON_DATA) 
FROM @my_internal_stage_for_apollo_json
FILE_FORMAT = (FORMAT_NAME = JSON_TYPE); 


SELECT * FROM APOLLO_VARIANT

-- STEP 7 : QUERING JSON DATA FROM THIS TABLE 


-- TILL THIS POINT WE HAVE THE JSON DATA IN SNOWFLAKE TABLE AS VARIANT 

-- THE BELOW STEPS WILL PARSE THE JSON AND PUT INTO DIFFERENT TABLES 

-- PART 1 

CREATE OR REPLACE TABLE STG_STATS_REALASSET 
(
    SESSION_ID VARCHAR,
    REPORT_ID VARCHAR,
    JOBID VARCHAR,
    JOB_STATUS VARCHAR,
    STATUS VARCHAR,
    MESSAGE VARCHAR,
    ROWCOUNT INTEGER,
    COLUMNCOUNT INTEGER,
    LIMIT INTEGER,
    SKIP INTEGER
);

INSERT INTO STG_STATS_REALASSET 
SELECT JSON_DATA:SessionId::string AS SessionId,
       JSON_DATA:ReportId::string AS ReportId,
       JSON_DATA:JobId::string AS JobId,
       JSON_DATA:JobStatus::string AS JobStatus,
       JSON_DATA:Status::string AS Status,
       JSON_DATA:Message::string AS Message,
       JSON_DATA:ReportOutput:RowCount::Integer AS RowCount,
       JSON_DATA:ReportOutput:ColumnCount::Integer AS ColumnCount,
       JSON_DATA:ReportOutput:Limit::Integer AS Limit,
       JSON_DATA:ReportOutput:Skip::Integer AS Skip
from APOLLO_VARIANT
;


SELECT * FROM STG_STATS_REALASSET;

-- PART 2 

CREATE OR REPLACE TABLE STG_METADATA_REALASSET
(
   REPORT_ID VARCHAR,
   ColumnId VARCHAR,
   ColumnName VARCHAR,
   BaseFieldName VARCHAR,
   DataType VARCHAR
);

INSERT INTO STG_METADATA_REALASSET
SELECT JSON_DATA:ReportId::string AS ReportId,
       Header.Value:ColumnId::string AS ColumnId,
       Header.Value:ColumnName::string AS ColumnName,
       Header.Value:BaseFieldName::string AS BaseFieldName,
       Header.Value:DataType::string AS DataType
from APOLLO_VARIANT,
LATERAL FLATTEN(INPUT => JSON_DATA:ReportOutput.ColumnHeaders) AS Header; 


SELECT * FROM STG_METADATA_REALASSET;


-- PART 3 

CREATE OR REPLACE TABLE STG_LOAN_REALASSET
(
"Report_id" VARCHAR,
"Portfolio" VARCHAR,
"Loan Name"  VARCHAR,
"APO Vehicle" VARCHAR,
"As Of Date" DATE,
"Funded USD" VARCHAR,
"Unfunded USD" VARCHAR,
"Committed USD" VARCHAR,
"Loan Currency" VARCHAR,
"APO Loan Position" VARCHAR,
"APO Underlying Asset Class" VARCHAR,
"Athene Source" VARCHAR,
"CM Rating" VARCHAR,
"Loan Orig Date" DATE,
"Expected Payoff Date" DATE,
"Current Maturity Date" DATE,
"Fully Extended Maturity Date" DATE,
"Rate Type" VARCHAR,
"Index" VARCHAR,
"Index Floor" VARCHAR,
"Spread" VARCHAR,
"Interest Rate" VARCHAR,
"LTV" VARCHAR,
"Pro Forma DSCR" VARCHAR,
"Pro Forma DY" VARCHAR,
"APO Financing" VARCHAR,
"APO Repo Lender" VARCHAR,
"Borrower CSV" VARCHAR,
"IGNORE ->" VARCHAR,
"APO Position and Rate Type" VARCHAR,
"Apollo Vehicle and Position" VARCHAR,
"Owner and Position" VARCHAR,
"Athene Portfolio" VARCHAR,
"Deal Name" VARCHAR,
"APO Property Type" VARCHAR,
"Origination Period" VARCHAR,
"Loan Number" VARCHAR,
"Tranche Name" VARCHAR,
"Owner" VARCHAR,
"Orig Loan Balance Local" VARCHAR,
"Orig Unfunded Local" VARCHAR,
"Orig Commitment Local" VARCHAR,
"Balance Date" VARCHAR,
"Loan Balance Local" VARCHAR,
"Unfunded Local" VARCHAR,
"Commitment Local" VARCHAR,
"Loan Type" VARCHAR,
"Deal Status" VARCHAR,
"Portfolio Sort Order" VARCHAR,
"Portfolio Name" VARCHAR,
"Loan Deal Rank" VARCHAR,
"Internal/ 3rd Party" VARCHAR,
"Portfolio Status" VARCHAR,
"APO Financing As Of Date" VARCHAR,
"LMS Loan ID" VARCHAR,
"LMS Deal Source" VARCHAR,
"LMS Asset Fee Type" VARCHAR,
"LMS CM Rating" VARCHAR,
"LMS CM Rating Date" VARCHAR,
"Financing" VARCHAR,
"Position" VARCHAR,
"Metrics as of Date" VARCHAR,
"APO DY Indicator" VARCHAR,
"APO DSCR Indicator" VARCHAR,
"APO NOI DY" VARCHAR,
"APO NOI DSCR" VARCHAR,
"APO NCF DY" VARCHAR,
"APO NCF DSCR" VARCHAR,
"Index Name" VARCHAR,
"APO AM" VARCHAR,
"Situs AM" VARCHAR,
"FX As Of Date" VARCHAR,
"GBP to USD FX Rate" VARCHAR,
"EUR to USD FX Rate" VARCHAR,
"SEK to USD FX Rate" VARCHAR,
"Funded USD (MM)" VARCHAR,
"Unfunded USD (MM)" VARCHAR,
"Committed USD (MM)" VARCHAR,
"Loan Count" VARCHAR,
"Apollo Investor" VARCHAR,
"APO Metrics Type" VARCHAR
);


INSERT INTO STG_LOAN_REALASSET
SELECT 
        JSON_DATA:ReportId::string AS ReportId,
        Records.Value:Portfolio::string AS Portfolio,
   	    Records.Value:"Loan Name"::string AS "Loan Name",
	    Records.Value:"APO Vehicle"::string AS "APO Vehicle",
	    Records.Value:"As Of Date"::string AS "As Of Date" ,
	    Records.Value:"Funded USD"::float AS "Funded USD",
	    Records.Value:"Unfunded USD"::float AS "Unfunded USD",
	    Records.Value:"Committed USD"::float AS "Committed USD",
		Records.Value:"Loan Currency"::string AS "Loan Currency",
		Records.Value:"APO Loan Position"::string AS "APO Loan Position",
		Records.Value:"APO Underlying Asset Class"::string AS "APO Underlying Asset Class",
		Records.Value:"Athene Source"::string AS "Athene Source",
		Records.Value:"CM Rating"::string AS "CM Rating",
		Records.Value:"Loan Orig"::string AS "Loan Orig",
		Records.Value:"Expected Payoff"::string AS "Expected Payoff",
		Records.Value:"Current Maturity Date"::string AS "Current Maturity Date",
		Records.Value:"Fully Extended Maturity Date"::string AS "Fully Extended Maturity Date",
		Records.Value:"Rate Type"::string AS "Rate Type",
		Records.Value:"Index"::string AS "Index",
		Records.Value:"Index Floor"::float AS "Index Floor",
		Records.Value:"Spread"::float AS "Spread",
		Records.Value:"Interest Rate"::float AS "Interest Rate",
		Records.Value:"LTV"::string AS "LTV",
		Records.Value:"Pro Forma DSCR"::float AS "Pro Forma DSCR",
		Records.Value:"Pro Forma DY"::float AS "Pro Forma DY",
		Records.Value:"APO Financing"::string AS "APO Financing",
		Records.Value:"APO Repo Lender"::string AS "APO Repo Lender",
		Records.Value:"Borrower CSV"::string AS "Borrower CSV",
		Records.Value:"IGNORE ->"::string AS "IGNORE ->",
		Records.Value:"APO Position and Rate Type"::string AS "APO Position and Rate Type",
		Records.Value:"Apollo Vehicle and Position"::string AS "Apollo Vehicle and Position",
		Records.Value:"Owner and Position"::string AS "Owner and Position",
		Records.Value:"Athene Portfolio"::string AS "Athene Portfolio",
		Records.Value:"Deal Name"::string AS "Deal Name",
		Records.Value:"APO Property Type"::string AS "APO Property Type",
		Records.Value:"Origination Period"::string AS "Origination Period",
		Records.Value:"Loan Number"::string AS "Loan Number",
		Records.Value:"Tranche Name"::string AS "Tranche Name",
		Records.Value:"Owner"::string AS "Owner",
		Records.Value:"Orig Loan Balance Local"::float AS "Orig Loan Balance Local",
		Records.Value:"Orig Unfunded Local"::float AS "Orig Unfunded Local",
		Records.Value:"Orig Commitment Local"::float AS "Orig Commitment Local",
		Records.Value:"Balance Date"::string AS "Balance Date",
		Records.Value:"Loan Balance Local"::float AS "Loan Balance Local",
		Records.Value:"Unfunded Local"::float AS "Unfunded Local",
		Records.Value:"Commitment Local"::float AS "Commitment Local",
		Records.Value:"Loan Type"::string AS "Loan Type",
		Records.Value:"Deal Status"::string AS "Deal Status",
		Records.Value:"Portfolio Sort Order"::string AS "Portfolio Sort Order",
		Records.Value:"Portfolio Name"::string AS "Portfolio Name",
		Records.Value:"Loan Deal Rank"::float AS "Loan Deal Rank",
		Records.Value:"Internal/ 3rd Party"::string AS "Internal/ 3rd Party",
		Records.Value:"Portfolio Status"::string AS "Portfolio Status",
		Records.Value:"APO Financing As Of Date"::string AS "APO Financing As Of Date",
		Records.Value:"LMS Loan ID"::string AS "LMS Loan ID",
		Records.Value:"LMS Deal Source"::string AS "LMS Deal Source",
		Records.Value:"LMS Asset Fee Type"::string AS "LMS Asset Fee Type",
		Records.Value:"LMS CM Rating"::string AS "LMS CM Rating",
		Records.Value:"LMS CM Rating Date"::string AS "LMS CM Rating Date",
		Records.Value:"Financing"::string AS "Financing",
		Records.Value:"Position"::string AS "Position",
		Records.Value:"Metrics as of Date"::string AS "Metrics as of Date",
		Records.Value:"APO DY Indicator"::string AS "APO DY Indicator",
		Records.Value:"APO DSCR Indicator"::string AS "APO DSCR Indicator",
		Records.Value:"APO NOI DY"::string AS "APO NOI DY",
		Records.Value:"APO NOI DSCR"::string AS "APO NOI DSCR",
		Records.Value:"APO NCF DY"::string AS "APO NCF DY",
		Records.Value:"APO NCF DSCR"::string AS "APO NCF DSCR",
		Records.Value:"Index Name"::string AS "Index Name",
		Records.Value:"APO AM"::string AS "APO AM",
		Records.Value:"Situs AM"::string AS "Situs AM",
		Records.Value:"FX As Of Date"::string AS "FX As Of Date",
		Records.Value:"GBP to USD FX Rate"::float AS "GBP to USD FX Rate",
		Records.Value:"EUR to USD FX Rate"::float AS "EUR to USD FX Rate",
		Records.Value:"SEK to USD FX Rate"::float AS "SEK to USD FX Rate",
		Records.Value:"Funded USD (MM)"::float AS "Funded USD (MM)",
		Records.Value:"Unfunded USD (MM)"::float AS "Unfunded USD (MM)",
		Records.Value:"Committed USD (MM)"::float AS "Committed USD (MM)",
		Records.Value:"Loan Count"::float AS "Loan Count",
		Records.Value:"Apollo Investor"::string AS "Apollo Investor",
   	    Records.Value:"APO Metrics Type"::string AS "APO Metrics Type"
      
from APOLLO_VARIANT,
LATERAL FLATTEN(INPUT => JSON_DATA:ReportOutput.Records) AS Records; 



SELECT * FROM STG_LOAN_REALASSET;
-------------------------------

-- *********************************************************************************************************************

CALL NEW_DB_OBJECT()

CREATE OR REPLACE PROCEDURE NEW_DB_OBJECT()
RETURNS VARCHAR
LANGUAGE SQL
AS
$$
DECLARE 
        
         error       variant default null;           
         result      varchar default ''''; 
         login       varchar default (select current_user()     )  ;
         wh_size     varchar default (select current_warehouse())  ;
         sess_id     varchar default (select current_session()  )  ;
         db_name     varchar default (select current_database() )  ;
         schema_name varchar default (select current_schema()   )  ; 
         step_name   varchar default 'procedure start' ;   
         proc_name   varchar default 'sp_load_change_capture';      
                   
BEGIN 
         
       TRUNCATE TABLE STG_STATS_REALASSET; 
       TRUNCATE TABLE STG_METADATA_REALASSET;
       TRUNCATE TABLE STG_LOAN_REALASSET;
       
       INSERT INTO STG_STATS_REALASSET 
              SELECT JSON_DATA:SessionId::string AS SessionId,
                  JSON_DATA:ReportId::string AS ReportId,
                  JSON_DATA:JobId::string AS JobId,
                  JSON_DATA:JobStatus::string AS JobStatus,
                  JSON_DATA:Status::string AS Status,
                  JSON_DATA:Message::string AS Message,
                  JSON_DATA:ReportOutput:RowCount::Integer AS RowCount,
                  JSON_DATA:ReportOutput:ColumnCount::Integer AS ColumnCount,
                  JSON_DATA:ReportOutput:Limit::Integer AS Limit,
                  JSON_DATA:ReportOutput:Skip::Integer AS Skip
             FROM APOLLO_VARIANT;


      
       
      INSERT INTO STG_METADATA_REALASSET
             SELECT JSON_DATA:ReportId::string AS ReportId,
                    Header.Value:ColumnId::string AS ColumnId,
                    Header.Value:ColumnName::string AS ColumnName,
                    Header.Value:BaseFieldName::string AS BaseFieldName,
                    Header.Value:DataType::string AS DataType
            FROM APOLLO_VARIANT,
            LATERAL FLATTEN(INPUT => JSON_DATA:ReportOutput.ColumnHeaders) AS Header; 


       INSERT INTO STG_LOAN_REALASSET
             SELECT 
                    JSON_DATA:ReportId::string AS ReportId,
                    Records.Value:Portfolio::string AS Portfolio,
   	                Records.Value:"Loan Name"::string AS "Loan Name",
	                Records.Value:"APO Vehicle"::string AS "APO Vehicle",
	                Records.Value:"As Of Date"::string AS "As Of Date" ,
	                Records.Value:"Funded USD"::float AS "Funded USD",
	                Records.Value:"Unfunded USD"::float AS "Unfunded USD",
	                Records.Value:"Committed USD"::float AS "Committed USD",
		            Records.Value:"Loan Currency"::string AS "Loan Currency",
		            Records.Value:"APO Loan Position"::string AS "APO Loan Position",
		            Records.Value:"APO Underlying Asset Class"::string AS "APO Underlying Asset Class",
		            Records.Value:"Athene Source"::string AS "Athene Source",
		            Records.Value:"CM Rating"::string AS "CM Rating",
		            Records.Value:"Loan Orig"::string AS "Loan Orig",
		            Records.Value:"Expected Payoff"::string AS "Expected Payoff",
		            Records.Value:"Current Maturity Date"::string AS "Current Maturity Date",
		            Records.Value:"Fully Extended Maturity Date"::string AS "Fully Extended Maturity Date",
	            	Records.Value:"Rate Type"::string AS "Rate Type",
		            Records.Value:"Index"::string AS "Index",
		            Records.Value:"Index Floor"::float AS "Index Floor",
		            Records.Value:"Spread"::float AS "Spread",
		            Records.Value:"Interest Rate"::float AS "Interest Rate",
	          	    Records.Value:"LTV"::string AS "LTV",
		            Records.Value:"Pro Forma DSCR"::float AS "Pro Forma DSCR",
		            Records.Value:"Pro Forma DY"::float AS "Pro Forma DY",
		            Records.Value:"APO Financing"::string AS "APO Financing",
		            Records.Value:"APO Repo Lender"::string AS "APO Repo Lender",
		            Records.Value:"Borrower CSV"::string AS "Borrower CSV",
		            Records.Value:"IGNORE ->"::string AS "IGNORE ->",
		            Records.Value:"APO Position and Rate Type"::string AS "APO Position and Rate Type",
		            Records.Value:"Apollo Vehicle and Position"::string AS "Apollo Vehicle and Position",
		            Records.Value:"Owner and Position"::string AS "Owner and Position",
		            Records.Value:"Athene Portfolio"::string AS "Athene Portfolio",
		            Records.Value:"Deal Name"::string AS "Deal Name",
		            Records.Value:"APO Property Type"::string AS "APO Property Type",
		            Records.Value:"Origination Period"::string AS "Origination Period",
		            Records.Value:"Loan Number"::string AS "Loan Number",
		            Records.Value:"Tranche Name"::string AS "Tranche Name",
		            Records.Value:"Owner"::string AS "Owner",
		            Records.Value:"Orig Loan Balance Local"::float AS "Orig Loan Balance Local",
		            Records.Value:"Orig Unfunded Local"::float AS "Orig Unfunded Local",
		            Records.Value:"Orig Commitment Local"::float AS "Orig Commitment Local",
		            Records.Value:"Balance Date"::string AS "Balance Date",
		            Records.Value:"Loan Balance Local"::float AS "Loan Balance Local",
		            Records.Value:"Unfunded Local"::float AS "Unfunded Local",
		            Records.Value:"Commitment Local"::float AS "Commitment Local",
		            Records.Value:"Loan Type"::string AS "Loan Type",
		            Records.Value:"Deal Status"::string AS "Deal Status",
		            Records.Value:"Portfolio Sort Order"::string AS "Portfolio Sort Order",
		            Records.Value:"Portfolio Name"::string AS "Portfolio Name",
		            Records.Value:"Loan Deal Rank"::float AS "Loan Deal Rank",
		            Records.Value:"Internal/ 3rd Party"::string AS "Internal/ 3rd Party",
		            Records.Value:"Portfolio Status"::string AS "Portfolio Status",
		            Records.Value:"APO Financing As Of Date"::string AS "APO Financing As Of Date",
		            Records.Value:"LMS Loan ID"::string AS "LMS Loan ID",
		            Records.Value:"LMS Deal Source"::string AS "LMS Deal Source",
		            Records.Value:"LMS Asset Fee Type"::string AS "LMS Asset Fee Type",
		            Records.Value:"LMS CM Rating"::string AS "LMS CM Rating",
		            Records.Value:"LMS CM Rating Date"::string AS "LMS CM Rating Date",
		            Records.Value:"Financing"::string AS "Financing",
		            Records.Value:"Position"::string AS "Position",
		            Records.Value:"Metrics as of Date"::string AS "Metrics as of Date",
		            Records.Value:"APO DY Indicator"::string AS "APO DY Indicator",
		            Records.Value:"APO DSCR Indicator"::string AS "APO DSCR Indicator",
		            Records.Value:"APO NOI DY"::string AS "APO NOI DY",
		            Records.Value:"APO NOI DSCR"::string AS "APO NOI DSCR",
		            Records.Value:"APO NCF DY"::string AS "APO NCF DY",
		            Records.Value:"APO NCF DSCR"::string AS "APO NCF DSCR",
		            Records.Value:"Index Name"::string AS "Index Name",
		            Records.Value:"APO AM"::string AS "APO AM",
		            Records.Value:"Situs AM"::string AS "Situs AM",
		            Records.Value:"FX As Of Date"::string AS "FX As Of Date",
		            Records.Value:"GBP to USD FX Rate"::float AS "GBP to USD FX Rate",
		            Records.Value:"EUR to USD FX Rate"::float AS "EUR to USD FX Rate",
		            Records.Value:"SEK to USD FX Rate"::float AS "SEK to USD FX Rate",
		            Records.Value:"Funded USD (MM)"::float AS "Funded USD (MM)",
		            Records.Value:"Unfunded USD (MM)"::float AS "Unfunded USD (MM)",
		            Records.Value:"Committed USD (MM)"::float AS "Committed USD (MM)",
		            Records.Value:"Loan Count"::float AS "Loan Count",
		            Records.Value:"Apollo Investor"::string AS "Apollo Investor",
   	                Records.Value:"APO Metrics Type"::string AS "APO Metrics Type"
      
        FROM APOLLO_VARIANT,
        LATERAL FLATTEN(INPUT => JSON_DATA:ReportOutput.Records) AS Records; 

RETURN 'SUCCESSFULLY UPDATED THE TABLES';
END;
$$


-- VALIDATE 


SELECT * FROM STG_STATS_REALASSET;
SELECT * FROM STG_METADATA_REALASSET;
SELECT * FROM STG_LOAN_REALASSET;

-- ------------------------------------------------------------

select * from STG_METADATA_REALASSET

select concat('"',COLUMNNAME,'"')AS COL FROM  STG_METADATA_REALASSET

SELECT CONCAT('"',COLUMNNAME,'"')AS COL FROM  STG_METADATA_REALASSET


LOWER(REPLACE(column_name, ' ', '_'))


update STG_METADATA_REALASSET
set COLUMNNAME = upper(TRIM(REPLACE(COLUMNNAME,' ','_')))

update STG_METADATA_REALASSET
set COLUMNNAME = upper(TRIM(REPLACE(COLUMNNAME,'(','')))

update STG_METADATA_REALASSET
set COLUMNNAME = upper(TRIM(REPLACE(COLUMNNAME,')','')))

update STG_METADATA_REALASSET
set COLUMNNAME = upper(TRIM(REPLACE(COLUMNNAME,'/','')))

update STG_METADATA_REALASSET
set COLUMNNAME = upper(TRIM(REPLACE(COLUMNNAME,'->','')))




-- Creating Dynamic Table 

CREATE OR REPLACE PROCEDURE CreateDynamicTable()
RETURNS STRING
LANGUAGE JAVASCRIPT
AS '
    // Declare the SQL statement to retrieve the column names
    var getColumnsSQL = "SELECT DISTINCT COLUMNNAME FROM STG_METADATA_REALASSET";

    
    // Execute the SQL statement to retrieve the column names
    var stmt = snowflake.createStatement({sqlText: getColumnsSQL});
    var resultSet = stmt.execute();

    
    var columnNames = [];
    while (resultSet.next()) {
        // Store the column names in an array
        columnNames.push(resultSet.getColumnValue(1));
    }
    
    // Create the dynamic SQL statement to create the new table
    var createTableSQL = "CREATE OR REPLACE TABLE DynamicTable (";
    createTableSQL += columnNames.map(function(COLUMNNAME) {
        return \"\"+COLUMNNAME+ \"\"+" VARCHAR";
    }).join(", ");
    createTableSQL += ")"; 
    
    // Execute the dynamic SQL statement
    stmt = snowflake.createStatement({sqlText: createTableSQL});
    stmt.execute();

       
    return "DynamicTable created successfully.";
';
--**********************

CREATE OR REPLACE PROCEDURE CreateDynamicTable2()
RETURNS STRING
LANGUAGE JAVASCRIPT
AS '
    // Declare the SQL statement to retrieve the column names
    var getColumnsSQL = `select concat(''"'',COLUMNNAME,''"'','' '',(CASE WHEN DATATYPE in (''text'',''money'',''percent'',''note'')then ''varchar'' WHEN DATATYPE IN (''date'') THEN ''DATE'' else DATATYPE end)  )AS COL FROM  STG_METADATA_REALASSET`;

    
    // Execute the SQL statement to retrieve the column names
    var stmt = snowflake.createStatement({sqlText: getColumnsSQL});
    var resultSet = stmt.execute();

    
    var columnNames = [];
    while (resultSet.next()) {
        // Store the column names in an array
        columnNames.push(resultSet.getColumnValue(1));
    }
    
    // Create the dynamic SQL statement to create the new table
    var createTableSQL = "CREATE OR REPLACE TABLE DynamicTable2 (";
    createTableSQL += columnNames.map(function(COL) {
        return \"\"+COL+ \"\";
    }).join(", ");
    createTableSQL += ")"; 
    
    // Execute the dynamic SQL statement
    stmt = snowflake.createStatement({sqlText: createTableSQL});
    stmt.execute();

       
    return "DynamicTable2 created successfully.";
';




-- *************

drop table DynamicTable2;

CALL CreateDynamicTable2();

UPDATE DynamicTable  
    SET COLUMNID = (SELECT COLUMNNAME FROM STG_METADATA_REALASSET A WHERE COLUMNID = A.COLUMNID)

select * from DynamicTable;

alter table DynamicTable rename column old_name to new_name;


insert into DynamicTable
SELECT 
       
        Records.Value:Portfolio::string AS Portfolio,
   	    Records.Value:"Loan Name"::string AS "Loan Name",
	    Records.Value:"APO Vehicle"::string AS "APO Vehicle",
	    Records.Value:"As Of Date"::string AS "As Of Date" ,
	    Records.Value:"Funded USD"::float AS "Funded USD",
	    Records.Value:"Unfunded USD"::float AS "Unfunded USD",
	    Records.Value:"Committed USD"::float AS "Committed USD",
		Records.Value:"Loan Currency"::string AS "Loan Currency",
		Records.Value:"APO Loan Position"::string AS "APO Loan Position",
		Records.Value:"APO Underlying Asset Class"::string AS "APO Underlying Asset Class",
		Records.Value:"Athene Source"::string AS "Athene Source",
		Records.Value:"CM Rating"::string AS "CM Rating",
		Records.Value:"Loan Orig"::string AS "Loan Orig",
		Records.Value:"Expected Payoff"::string AS "Expected Payoff",
		Records.Value:"Current Maturity Date"::string AS "Current Maturity Date",
		Records.Value:"Fully Extended Maturity Date"::string AS "Fully Extended Maturity Date",
		Records.Value:"Rate Type"::string AS "Rate Type",
		Records.Value:"Index"::string AS "Index",
		Records.Value:"Index Floor"::float AS "Index Floor",
		Records.Value:"Spread"::float AS "Spread",
		Records.Value:"Interest Rate"::float AS "Interest Rate",
		Records.Value:"LTV"::string AS "LTV",
		Records.Value:"Pro Forma DSCR"::float AS "Pro Forma DSCR",
		Records.Value:"Pro Forma DY"::float AS "Pro Forma DY",
		Records.Value:"APO Financing"::string AS "APO Financing",
		Records.Value:"APO Repo Lender"::string AS "APO Repo Lender",
		Records.Value:"Borrower CSV"::string AS "Borrower CSV",
		Records.Value:"IGNORE ->"::string AS "IGNORE ->",
		Records.Value:"APO Position and Rate Type"::string AS "APO Position and Rate Type",
		Records.Value:"Apollo Vehicle and Position"::string AS "Apollo Vehicle and Position",
		Records.Value:"Owner and Position"::string AS "Owner and Position",
		Records.Value:"Athene Portfolio"::string AS "Athene Portfolio",
		Records.Value:"Deal Name"::string AS "Deal Name",
		Records.Value:"APO Property Type"::string AS "APO Property Type",
		Records.Value:"Origination Period"::string AS "Origination Period",
		Records.Value:"Loan Number"::string AS "Loan Number",
		Records.Value:"Tranche Name"::string AS "Tranche Name",
		Records.Value:"Owner"::string AS "Owner",
		Records.Value:"Orig Loan Balance Local"::float AS "Orig Loan Balance Local",
		Records.Value:"Orig Unfunded Local"::float AS "Orig Unfunded Local",
		Records.Value:"Orig Commitment Local"::float AS "Orig Commitment Local",
		Records.Value:"Balance Date"::string AS "Balance Date",
		Records.Value:"Loan Balance Local"::float AS "Loan Balance Local",
		Records.Value:"Unfunded Local"::float AS "Unfunded Local",
		Records.Value:"Commitment Local"::float AS "Commitment Local",
		Records.Value:"Loan Type"::string AS "Loan Type",
		Records.Value:"Deal Status"::string AS "Deal Status",
		Records.Value:"Portfolio Sort Order"::string AS "Portfolio Sort Order",
		Records.Value:"Portfolio Name"::string AS "Portfolio Name",
		Records.Value:"Loan Deal Rank"::float AS "Loan Deal Rank",
		Records.Value:"Internal/ 3rd Party"::string AS "Internal/ 3rd Party",
		Records.Value:"Portfolio Status"::string AS "Portfolio Status",
		Records.Value:"APO Financing As Of Date"::string AS "APO Financing As Of Date",
		Records.Value:"LMS Loan ID"::string AS "LMS Loan ID",
		Records.Value:"LMS Deal Source"::string AS "LMS Deal Source",
		Records.Value:"LMS Asset Fee Type"::string AS "LMS Asset Fee Type",
		Records.Value:"LMS CM Rating"::string AS "LMS CM Rating",
		Records.Value:"LMS CM Rating Date"::string AS "LMS CM Rating Date",
		Records.Value:"Financing"::string AS "Financing",
		Records.Value:"Position"::string AS "Position",
		Records.Value:"Metrics as of Date"::string AS "Metrics as of Date",
		Records.Value:"APO DY Indicator"::string AS "APO DY Indicator",
		Records.Value:"APO DSCR Indicator"::string AS "APO DSCR Indicator",
		Records.Value:"APO NOI DY"::string AS "APO NOI DY",
		Records.Value:"APO NOI DSCR"::string AS "APO NOI DSCR",
		Records.Value:"APO NCF DY"::string AS "APO NCF DY",
		Records.Value:"APO NCF DSCR"::string AS "APO NCF DSCR",
		Records.Value:"Index Name"::string AS "Index Name",
		Records.Value:"APO AM"::string AS "APO AM",
		Records.Value:"Situs AM"::string AS "Situs AM",
		Records.Value:"FX As Of Date"::string AS "FX As Of Date",
		Records.Value:"GBP to USD FX Rate"::float AS "GBP to USD FX Rate",
		Records.Value:"EUR to USD FX Rate"::float AS "EUR to USD FX Rate",
		Records.Value:"SEK to USD FX Rate"::float AS "SEK to USD FX Rate",
		Records.Value:"Funded USD (MM)"::float AS "Funded USD (MM)",
		Records.Value:"Unfunded USD (MM)"::float AS "Unfunded USD (MM)",
		Records.Value:"Committed USD (MM)"::float AS "Committed USD (MM)",
		Records.Value:"Loan Count"::float AS "Loan Count",
		Records.Value:"Apollo Investor"::string AS "Apollo Investor",
   	    Records.Value:"APO Metrics Type"::string AS "APO Metrics Type"
      
from APOLLO_VARIANT,
LATERAL FLATTEN(INPUT => JSON_DATA:ReportOutput.Records) AS Records;




EXPERIMENT

CALL main_proc('APOLLO.APOLLO_SCHEMA.APOLLO_VARIANT')


CREATE OR REPLACE PROCEDURE main_proc(src_table TEXT)
RETURNS VARIANT
LANGUAGE JAVASCRIPT
EXECUTE AS CALLER
AS
$$
	// variable Declaration
	var formatted_pk = [];
    var stmt = snowflake.createStatement(
		{
			sqlText: 'select * from (:1) ',
			binds: [SRC_TABLE]
		}
	);
	
	// Snowflake statement execution part
	try{
		var result = stmt.execute();
		result.next();
		formatted_pk.push(result.getColumnValue(1))
	}
	
	// Exception handling part
	catch(err){
		return "Error :" + err;
	}
$$
;

CREATE OR REPLACE PROCEDURE DynamicInsertQuery(TABLENAME VARCHAR)
RETURNS VARIANT NOT NULL
LANGUAGE JAVASCRIPT
AS
$$
var my_sql_command = "INSERT INTO STG_STATS_REALASSET SELECT JSON_DATA:SessionId,JSON_DATA:ReportId,JSON_DATA:JobId,JSON_DATA:JobStatus,JSON_DATA:Status,JSON_DATA:Message FROM " +TABLENAME;
var statement1 = snowflake.createStatement({sqlText:my_sql_command});
var rs = statement1.execute();
return_code =  rs.getSqlcode();

if (return_code == 0){
    return 'Successfully inserted into STG_STATS_REALASSET ';}
else{
    return 'Error !! STG_STATS_REALASSET';
}



var my_sql_command2 = "INSERT INTO STG_METADATA_REALASSET SELECT Header.Value:ColumnId::string AS ColumnId,Header.Value:ColumnName::string AS ColumnName,Header.Value:BaseFieldName::string AS BaseFieldName,Header.Value:DataType::string AS DataType FROM " +TABLENAME+", LATERAL FLATTEN(INPUT => JSON_DATA:ReportOutput.ColumnHeaders) AS Header "; 

var statement2 = snowflake.createStatement({sqlText:my_sql_command2});
var rs = statement2.execute();
return_code =  rs.getSqlcode();

if (return_code == 0){
    return 'Successfully inserted into STG_METADATA_REALASSET ';}
else{
    return 'Error !! STG_METADATA_REALASSET';
}

$$;


CALL DynamicInsertQuery('APOLLO_VARIANT')

select * from STG_METADATA_REALASSET

"INSERT INTO STG_STATS_REALASSET SELECT JSON_DATA:SessionId,JSON_DATA:ReportId,JSON_DATA:JobId,JSON_DATA:JobStatus,JSON_DATA:Status,JSON_DATA:Message FROM +TABLENAME";


var my_sql_command = "update "+TABLENAME+" set "+COLUMNNAME1+"='"+VALUE1+"' where "+COLUMNNAME2+"='"+VALUE2+"' ";

"INSERT INTO STG_METADATA_REALASSET SELECT Header.Value:ColumnId::string AS ColumnId,Header.Value:ColumnName::string AS ColumnName,Header.Value:BaseFieldName::string AS BaseFieldName,Header.Value:DataType::string AS DataType FROM " +TABLENAME" ,         LATERAL FLATTEN(INPUT => JSON_DATA:ReportOutput.ColumnHeaders) AS Header "; 







































        









                     
                      

     
      
      










                               






















 



























