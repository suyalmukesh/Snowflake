Data can be transformed when loading data  ( Simplfy ETL pipelines ) 

Supported

    1. Column reordering 
    2. Cast data types
    3. Remove columns 
    4. Truncate (TRUNCATECOLUMNS)
    5. Subset of SQL functions 

Not Supported 

    1. FLATTEN functions 
    2. Aggregation functions
    3. GROUP by
    4. Filter with WHERE 
    5. JOINS 

--------------------------------------------------------------------------------------------------------------------------------------
*** Functions **** 
------------------
Supports most standard SQL functions defined in SQL:1999 and parts of SQL:2003 extensions

1. Scalar Functions : Returns one value per invocation ( one value per row )

    SELECT DAYNAME('2023-12-31)    
    -- DAYNAME('2023-12-31)  
    -- SUN

   SELECT DAYNAME("effective_date") FROM LOAN_PAYMENT 
    -- DAYNAME("effective_date")
    -- 1 THU
    -- 2 THU
    -- 3 FRI


2.  Aggregate functions - Mathematical calculations such as MAX and MIN across rows


3.  Window functions - Aggregate functions that operate on a subset of rows 


4.  Table functions - Return a set of rows for each input row - used to obtain information about snowflake features 
    SELECT * FROM TABLE(VALIDATE(ORDERS , JOB_ID => '_last'))


5.  System Functions - Control & information functions - 
      usually SYSTEM$ ... 
              SYSTEM$CANCEL_ALL_QUERIES 
        SELECT SYSTEM$TYPEOF('abc')      


6.  UDFs + EXternal - Define functions - Store & execute outside of Snowflake 


7.  Estimating Functions

The idea : 
Exact calculations on very large tables can be very compute-/ memory-intensive. 

Mathematical algorithms can approximate the exact number , they might be good enough and require fewer resources.

   1.  Number of Distinct Values : HLL()
       Idea : HyperLogLog (cardinality estimation algorithm ) is used to estimate the number of distinct values . State of the art mathematical algorithm 

       Situation :  COUNT(DISTINCT(COL1,..))
       When there is very large input and Average error is acceptable 

       HLL(COL1,..)  . This is exactly same as if we are using APPROX_COUNT_DISTINCT(COL1,....) 

       EXAMPLES :
       SELECT HLL(C_NAME) FROM SNOWFLAKE_SAMPLE_DATA.TPCH.SF1000_CUSTOMER;   <very less time >

       SELECT COUNT(DISTINCT(C_NAME)) FROM  SNOWFLAKE_SAMPLE_DATA.TPCH.SF1000_CUSTOMER;   <more time >    



   2.  Frequent Values : APPROX_TOP_K()
       Idea : Space-Saving algorithm is used to estimate the most frequent values along with their frequency . To estimate top k values 

       APPROX_TOP_K (COLUMN)
       APPROX_TOP_K (COLUMN, <K>)  -- DEFAULT K = 1 , K = NO OF VALUES WHOSE FREQUENCY SHOULD BE APPROXIMATED 
       APPROX_TOP_K (COLUMN, <K>, <counters>)  -- counters = Max num of distinct values that can be tracked      count >> k   large count means more accurate


       Example : a large table 28.8B rows 

       SELECT SS_CUSTOMER_SK , COUNT(SS_CUSTOMER_SK)
       FROM STORE_SALES 
       GROUP BY SS_CUSTOMER_SK ;   (Took around 18 minutes )

       SELECT APPROX_TOP_K (SS_CUSTOMER_SK,5,20) FROM STORE_SALES;  (arrox 3 mins )

       Result is like a Array :

      --  [[20952969,174160540],[44412221,174160540],[30594069,174160530]...]

   3.  Percentile Values : APPROX_PERCENTILE()
 
       Idea : t-Digest algorithm is used to estimate percentile values . 
      
      APPROX_PERCENTILE(COLUMN,<percentile>)  -- Return the percentile value 

      SELECT APPROX_PERCENTILE(O_TOTALPRICE,0.5) FROM ORDERS; -- This will give us the value in the middle , all the values are ordered and return the middle one 
      -- took 20 seconds

      whereas if we need exact value and use normal function it will take huge time as -

      SELECT PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY O_TOTALPRICE) FROM ORDERS;
      -- took around 2 mins 

 
 
   4.  Similarity of Two or More Sets : MINHASH + APPROXIMATE_SIMILARITY()

       Idea : Used MinHash to estimate the similarity between two or more data sets . 

       Traditionally this used to be done using Jaccard similarity coefficient is used to compare similarity .

       J(A,B)  = (A intersection B) / (A Union B)

       Now this intersection calculation is very intensive so snowflake has used MinHash which can extimate J(A,B) quickly . 


       So this is a two step process - 
         STEP 1. MINHASH -
              SELECT MINHASH(100,*) AS mh FROM mhtab1;
              SELECT MINHASH(100,*) AS mh FROM mhtab2;

            MINHASH returns a MinHash state . K is # of hash functions the larger k , the more accurate
        
            Example :  SELECT MINHASH(7,O_ORDERKEY) AS mh FROM ORDERS;
            {[
            MH 
              { 
                "state" : [
                    76767676767676,
                    888787897878787,
                    565757858757575

                ],
                "type":"minhash",
                "version":1
              }] }
        STEP 2: APPROXIMATE_SIMILARITY 

              SELECT APPROXIMATE_SIMILARITY(mh)
              FROM 
               ((SELECT MINHASH(100,*) AS mh FROM MHTAB1)
               UNION ALL 
               (SELECT MINHASH(100,*) AS mh FROM mhtab2));

        Use MinHash states to calculate similarity with APPROXIMATE_SIMILARITY(). 

        --  MINHASH OUTPUT IS LIKE 
        --  MH
        --  {"state": [200909090909090,67676767676767,87878787]}
        --  {"state": [888686868686,868686868686,7879787]}       
 
        AND THE FINAL OUTPUT RETURNED WILL BE VALUE BETWEEN 0 AND 1 . 
        0 means the datesets are very very diffeent
        1 means they match (identical)
        0 0.01 ...... .... 1




 


















